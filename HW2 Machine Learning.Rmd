---
title: "Machine Learning HW2"
author: "Devante Lee1"
date: "9/26/2020"
output: pdf_document
---

Bank_subset <- UniversalBank[c("Age","Experience", "Income", "Family", "CCAvg", "Education", "Mortgage","PersonalLoan", "SecuritiesAccount", "CDAccount", "Online",  "CreditCard")]


Bank_subset$PersonalLoan[Bank_subset$PersonalLoan == 0] <- 'No'
Bank_subset$PersonalLoan[Bank_subset$PersonalLoan == 1] <- 'Yes'

Bank_subset$PersonalLoan <- factor(Bank_subset$PersonalLoan)

ind <- sample(2, nrow(Bank_subset), replace = T, prob = c(.6, .4))

trainingd <-Bank_subset[ind== 1, ]
validationd <- Bank_subset[ind== 2, ]

set.seed(123)

tr.control<- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 3)

set.seed(222)
fit <- train(PersonalLoan ~ .,
             data = trainingd,
             method = 'knn',
             tuneLength = 20,
             trControl = tr.control,
             preProc = c("center", "scale"))



pred <- predict(fit, newdata = trainingd)

fit1 <- train(PersonalLoan ~ .,
             data = validationd,
             method = 'knn',
             tuneLength = 20,
             trControl = tr.control,
             preProc = c("center", "scale"))

pred1 <- predict(fit, newdata = validationd)

confusionMatrix(pred, trainingd$PersonalLoan)

confusionMatrix(pred1, validationd$PersonalLoan)

##Part 2

install.packages()

library(caret)
library(class)

Bank_subset <- UniversalBank[c("Age","Experience", "Income", "Family", "CCAvg", "Education", "Mortgage","PersonalLoan", "SecuritiesAccount", "CDAccount", "Online",  "CreditCard")]


Bank_subset$PersonalLoan[Bank_subset$PersonalLoan == 0] <- 'No'
Bank_subset$PersonalLoan[Bank_subset$PersonalLoan == 1] <- 'Yes'

Bank_subset$PersonalLoan <- factor(Bank_subset$PersonalLoan)

ind1 <- sample(3, nrow(Bank_subset), replace = T, prob = c(.5, .3, .2))

trainingd <-Bank_subset[ind1== 1, ]
validationd <- Bank_subset[ind1== 2, ]
testingd <- Bank_subset[ind1==3, ]

set.seed(123)

tr.control<- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 3)

set.seed(222)
fit <- train(PersonalLoan ~ .,
             data = trainingd,
             method = 'knn',
             tuneLength = 20,
             trControl = tr.control,
             preProc = c("center", "scale"))



pred <- predict(fit, newdata = trainingd)

fit1 <- train(PersonalLoan ~ .,
             data = validationd,
             method = 'knn',
             tuneLength = 20,
             trControl = tr.control,
             preProc = c("center", "scale"))

pred1 <- predict(fit1, newdata = validationd)

fit2 <- train(PersonalLoan ~ .,
             data = testingd,
             method = 'knn',
             tuneLength = 20,
             trControl = tr.control,
             preProc = c("center", "scale"))
        

Predictedvalues <- train(PersonalLoan ~ .,
             data = combineddata,
             method = 'knn',
             tuneLength = 20,
             trControl = tr.control,
             preProc = c("center", "scale"),
             tuneGrid = expand.grid(k = 5))

combineddata <- as.data.frame(trainingd , testingd)

combinedpred <- predict(Predictedvalues, newdata = combineddata)

pred2<- predict(fit2, newdata = testingd)

confusionMatrix(pred, trainingd$PersonalLoan)

confusionMatrix(pred1, validationd$PersonalLoan)

confusionMatrix(combinedpred, combineddata$PersonalLoan)


plot(fit)


# a) According to our data this customer would be classified as a loan not accepted.

# b) The values of K that balances between over fitting and underfitting is 9. 5 is the optimal value as it minimizes the error in the data allowing this to show the highest level of accuracy. This data is on the realm of being overfitted to the data as the model fits this data a bit too well. The accuracy of the data also decreases as we run our testing and validation data.I know this is not always the best indicator but we cannot ignore this in this data sample.

# c) This data set balances between recall and precision. I feel this model leans more toward the former error, sensitivity (recall) which is extremely high which is maximizing the models ablity to find relevant patterns throughout the data. Accuracy being extremely high is also not always an indicator of a great model and performance of the model. A very large number of the data points are also in one category (True negative). This is one of the main things to look for in your model possibly having some classification issues.

# d) The differences among the three different data sets is that the validation data provides the highest level of accuracy. This has the highest accuracy levels due to the large amount True Negatives and True positives inside of this data set.The training data based on the differerent K accuracy points is not as accurate as the validation model in terms of accuracy of the model. When the sets are added together the accuracy seems to incerease about but stays in the middle range with the added data that was inputed into the data fram.